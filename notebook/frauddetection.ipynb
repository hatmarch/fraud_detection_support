{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Sections of the notebook marked \"Train\" are used to train the model in the deployment pipeline.  All other cells will be excluded from the build of the model.\n",
    "\n",
    "Train cells should include the binary saving out of the model (via joblib) as a \"pickle\" for reading in later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Train"
    ]
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume a file data source\n",
    "dataSource = 'File'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable intelliense\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(train_labels, train_pred):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    labels = list(train_labels['Class'].value_counts().index)\n",
    "    desc_labels = ['Legit', 'Fraud']\n",
    "    #print(labels)\n",
    "\n",
    "    confusion = confusion_matrix(train_labels, train_pred, labels=labels)\n",
    "    ax.matshow(np.log(confusion + 1.001))\n",
    "\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "\n",
    "    ax.set_xticklabels(desc_labels, rotation=90);\n",
    "    ax.set_yticklabels(desc_labels);\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):        \n",
    "            ax.text(j, i, confusion[i,j], va='center', ha='center').set_backgroundcolor(\"white\")\n",
    "\n",
    "    plt.xlabel('Predicted')    \n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Input: \"Kernel Restart & Run All\" will block until the \"Proceed\" button is clicked after input selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to 'pip3 install ipython_blocking' for blocking to work\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import ipython_blocking\n",
    "\n",
    "selectw = widgets.RadioButtons(\n",
    "    options=['File', 'Spark', 'VDB'],\n",
    "    description='Select Input:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description='Proceed',layout= widgets.Layout(border='1px solid black'))\n",
    "\n",
    "box = widgets.VBox(children=[selectw, button], layout=widgets.Layout(border='solid', width='240px', height='160px'))\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Magic to Block Until \"Proceed\" Button Clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%blockrun button\n",
    "dataSource = selectw.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Based On Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Train"
    ]
   },
   "outputs": [],
   "source": [
    "if dataSource == 'Spark':\n",
    "    print(\"Spark input not yet implemented. Switching to use: File\")\n",
    "    dataSource = 'File'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Train"
    ]
   },
   "outputs": [],
   "source": [
    "if dataSource == 'VDB':\n",
    "    import jaydebeapi\n",
    "    \n",
    "    # The Vitual Database or VDB was created using Red Hat Data Virtualization.\n",
    "    # The VDB provides a virtual view of credit card fraud data consolidated from 2 sources:\n",
    "    # 1. MySQL (historical data from data warehouse)\n",
    "    # 2. A CSV file (recent data)\n",
    "    \n",
    "    # set up VDB access\n",
    "    # ****************** replace IP address with your VDB service cluster IP\n",
    "    url = \"jdbc:teiid:csvrdbmsdb.1@mm://172.30.126.110:31000\"\n",
    "    driver = \"org.teiid.jdbc.TeiidDriver\"\n",
    "    user = \"user\"\n",
    "    password = \"mypassword\"\n",
    "    jarfile = \"jdbc/teiid-9.0.6-jdbc.jar\"\n",
    "    conn = jaydebeapi.connect(driver, url, [user, password], jarfile)\n",
    "    SQL_Query = \"select * from CreditFraud\"\n",
    "\n",
    "    df = pd.read_sql_query(SQL_Query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # rename specific columns\n",
    "    df.rename(columns={'Time_':'Time', 'Class_':'Class'}, inplace=True)\n",
    "    \n",
    "    print(\"Data loaded using VDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Train"
    ]
   },
   "outputs": [],
   "source": [
    "if dataSource == 'File':\n",
    "\n",
    "    print(\"Reading csv data\")\n",
    "    df = pd.read_csv (\"data/creditcard.csv\")\n",
    "    # rename specific columns\n",
    "    df.rename(columns={'Unnamed: 0':'Id'}, inplace=True)\n",
    "    \n",
    "    print(\"Data loaded using File.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Display some info about the data\n",
    "print(\"Total number of credit card transaction rows: %d\" % df.shape[0])\n",
    "\n",
    "### Check the total number of rows with fraud detected\n",
    "print(\"Total number of rows with fraud: %d\" % df[(df['Class']==1)].shape[0])\n",
    "\n",
    "### Rough check if data columns have consistent datatype (not Object)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sklearn Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Train"
    ]
   },
   "outputs": [],
   "source": [
    "# shuffle the rows\n",
    "df = df.sample(frac=1).reset_index()\n",
    "\n",
    "#Order the credit card transaction by transaction time\n",
    "#df.sort_values(by=['Time'])\n",
    "\n",
    "#number of rows in the dataset\n",
    "n_samples = df.shape[0]\n",
    "print(\"Samples: %d\" % n_samples)\n",
    "\n",
    "#Split into train and test\n",
    "train_size = 0.75\n",
    "\n",
    "train_limit = int(n_samples * train_size)\n",
    "df_train = df.iloc[:train_limit]\n",
    "df_test = df.iloc[train_limit:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Train"
    ]
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "print('Training sample size: %d' % df_train.shape[0])\n",
    "frauds_in_train_tx = df_train[df_train.Class == 1].shape[0]\n",
    "print('Frauds transactions in training sample: %f' % (frauds_in_train_tx / train_limit))\n",
    "print('Testing sample size: %d' % df_test.shape[0])\n",
    "frauds_in_test_tx = df_test[df_test.Class == 1].shape[0]\n",
    "print('Frauds transactions in testing sample: %f' % (frauds_in_test_tx / (n_samples - train_limit)))\n",
    "      \n",
    "features=['V25', 'V28', 'V13', 'V24', 'V26', 'V22', 'V15']\n",
    "non_features = [i for i in df_train.columns if i not in features]\n",
    "\n",
    "#Define features and target variables for convenience.\n",
    "drop_columns = ['Id', 'Time', 'Class', 'V16','V4','V10','V11','V12','V14','V17', 'V7', 'V9', 'V2']\n",
    "select_column=['Class']\n",
    "\n",
    "#Create Train Datasets\n",
    "features_train = df_train.drop(non_features, axis=1)\n",
    "target_train = df_train.loc[:, select_column]\n",
    "#print(\"*Data Columns in features_train:\")\n",
    "#print(features_train.columns)\n",
    "#print(\"*Data Columns in target_train:\")\n",
    "#print(target_train.columns)\n",
    "\n",
    "\n",
    "#Create Test Datasets\n",
    "features_test = df_test.drop(non_features, axis=1)\n",
    "target_test = df_test.loc[:, select_column]\n",
    "\n",
    "#Create a RandomForest Classifier mode\n",
    "model = RandomForestClassifier(n_estimators=10, max_depth=5, n_jobs=10, class_weight='balanced')\n",
    "\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train)\n",
    "pred_test = model.predict(features_test)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train)\n",
    "pred_test_prob = model.predict_proba(features_test)\n",
    "\n",
    "print(\"Number of features: %d\" % len(model.feature_importances_))\n",
    "\n",
    "  \n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Model accuracy: %5.4f\" % accuracy_score(target_test, pred_test))\n",
    "\n",
    "#save mode in filesystem\n",
    "joblib.dump(model, 'model.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_=plot_confusion_matrix(target_train, model.predict(features_train))\n",
    "\n",
    "_=plot_confusion_matrix(target_test, model.predict(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "feat_imp = sorted(zip(features_train.columns, model.feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
    "X = [i[0] for i in feat_imp]\n",
    "Y = [i[1] for i in feat_imp]\n",
    "plt.figure(figsize=(10.,10.))\n",
    "\n",
    "# plt.plot([i[0] for i in feat_imp], [i[1] for i in feat_imp], 'p-')\n",
    "# sorts the X axis by the label name resulting in a weird-looking plot\n",
    "plt.plot(Y, 'p-')\n",
    "_ = plt.xticks(range(len(Y)), X, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Re-create the model with Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatively, Perform a Correlation Analysis \n",
    "We will come to a similar conclusion on which features to include in the model: those with a low correlation coefficient should be excluded to avoid overfitting.\n",
    "The graph has the features with the highest correlation coefficent plotted first. Class is to be excluded as it is the target for the correlation.\n",
    "This shows the importance of data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.loc[:, 'Class']\n",
    "\n",
    "dfc = df.corrwith(corr).to_frame()\n",
    "# use a temporary column to sort and then discard the column\n",
    "dfc = (dfc.assign(A=abs(dfc))\n",
    "    .sort_values(['A'],ascending=False)\n",
    "    .drop('A', 1))\n",
    "#print(dfc)\n",
    "dfc.plot.bar(\n",
    "        figsize = (20, 10), title = \"Correlation with Class\", fontsize = 15,\n",
    "        rot = 45, grid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-create the model with Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#Define features and target variables.\n",
    "\n",
    "# Choose either the features definition based on important features or correlation\n",
    "\n",
    "## From the important features graph we only want seven important features:\n",
    "##   V3,V4,V10,V11,V12,V14,V17\n",
    "features = ['V16','V4','V10','V11','V12','V14','V17']\n",
    "\n",
    "## using the correllation graph, select the top seven important features:\n",
    "## V14,V17,V3,V12,V10,V16,V11\n",
    "#features = ['V14','V17','V3','V12','V10','V16','V11']\n",
    "\n",
    "non_features = [i for i in df_train.columns if i not in features]\n",
    "class_column=['Class']\n",
    "\n",
    "\n",
    "features_train = df_train.drop(non_features, axis=1)\n",
    "target_train = df_train.loc[:, \"Class\"]\n",
    "\n",
    "features_test = df_test.drop(non_features, axis=1)\n",
    "target_test = df_test.loc[:, \"Class\"]\n",
    "print(\"feature_test columns:\")\n",
    "print(features_test.columns)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1000, max_depth=20, n_jobs=10, class_weight='balanced')\n",
    "\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train)\n",
    "pred_test = model.predict(features_test)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train)\n",
    "pred_test_prob = model.predict_proba(features_test)\n",
    "\n",
    "print(\"Number of features\")\n",
    "print(len(model.feature_importances_))\n",
    "  \n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Model accuracy: %5.4f\" % accuracy_score(target_test, pred_test))\n",
    " \n",
    "#save mode in filesystem\n",
    "joblib.dump(model, 'model.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(target_train.to_frame(), model.predict(features_train))\n",
    "\n",
    "_ = plot_confusion_matrix(target_test.to_frame(), model.predict(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "fraudTest = df_test.loc[df_test['Class']== 1]\n",
    "notFraudTest = df_test.loc[df_test['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=non_features)\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=non_features)\n",
    "\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    rowdf = pd.DataFrame([data.tolist()], columns = features)\n",
    "    print(model.predict(rowdf))\n",
    "    # time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model output\n",
    "\n",
    "This section can be used for testing the model once it is deployed.  It mimics the input and output expected from the [Seldon model](https://github.com/kalefranz/seldon-python-microservice/blob/ee6b49f7670ed8ecb04912400333db6781ee4110/seldon_microservice/model_microservice.py) that ultimately wraps the deployment (see below).  This is tagged to be part of the training output so that the results here can be compared to what's deployed for debugging purposes.\n",
    "\n",
    "Results should be *similar* to what it output here if the model is (re)trained in the pipeline.  This is due to the testing data being shuffled (see above code block) and also due to the nature of the training process for the RandomForestClassifier.\n",
    "\n",
    "If, however, the model that is saved out above is the exact one that goes into the pipeline, then the results should be identical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Train"
    ]
   },
   "outputs": [],
   "source": [
    "def get_class_names(user_model,n_targets):\n",
    "    if hasattr(user_model,\"class_names\"):\n",
    "        return user_model.class_names\n",
    "    else:\n",
    "        return [\"t:{}\".format(i) for i in range(n_targets)]\n",
    "    \n",
    "def array_to_rest_datadef(array,names):\n",
    "    datadef = {\"names\":names}\n",
    "    datadef[\"ndarray\"] = array.tolist()\n",
    "    return datadef\n",
    "\n",
    "loadedModel = joblib.load('model.pkl')\n",
    "print (\"Loaded model is: \", loadedModel)\n",
    "\n",
    "loadedModel.class_names = ['V3','V4','V10','V11','V12','V14','V17']\n",
    "X=[[-4.304597,  4.732795, -2.447469,  2.101344, -4.609628, -6.079337,  6.739384]]\n",
    "Y=np.array(X)\n",
    "\n",
    "print (\"X is: \", X)\n",
    "print (\"Prediction from model for X is: \", loadedModel.predict_proba(X))\n",
    "print (\"Y (numpy array) is: \", Y)\n",
    "print (\"Prediction from model for Y is: \", loadedModel.predict_proba(Y))\n",
    "pred = np.array(loadedModel.predict_proba(Y))\n",
    "print (\"Prediction for Y (in Seldon return format) is: \", pred)\n",
    "print (\"pred.shape is: \", pred.shape)\n",
    "print (\"Output from model for predict on Y is expected to look like this: \",  array_to_rest_datadef(pred, get_class_names(loadedModel, pred.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seldon Deployment\n",
    "\n",
    "This section will be extracted to represent the model being loaded in a format that is compatible with Seldon Core as outlined [in the documentation](https://docs.seldon.io/projects/seldon-core/en/v1.2.0/python/python_component.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Wrapper"
    ]
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "class Wrapper(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = joblib.load('model.pkl')\n",
    "        self.class_names = ['V3','V4','V10','V11','V12','V14','V17']\n",
    "        print (\"Model loaded.  Initially is:\", self.model)\n",
    "\n",
    "    def predict(self,X,features_names):\n",
    "        print (\"Loaded model is: \", self.model)\n",
    "        print (\"X is:\")\n",
    "        print (X)\n",
    "        print (\"features_names is: \")\n",
    "        print (features_names)\n",
    "        prediction = self.model.predict_proba(X)\n",
    "        print (\"output of predict_proba is: \", prediction)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git Helper\n",
    "\n",
    "This section is to aid in checking in changes to the notebook.  Uncomment and run the commands below to checkin all changes made to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd ..\n",
    "# !git pull origin\n",
    "# !git status\n",
    "# !git commit -am\"New Deploy\"\n",
    "# !git push origin"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}